{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't think you need it as long as this notebook is in the same folder as peerselect. \n",
    "import sys\n",
    "sys.path.insert(0, \"\\peerselect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes it easier to upodate changes without restarting the notebook. Ignore.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Standard Magic and startup initializers.\n",
    "\n",
    "import math\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from collections import Counter\n",
    "\n",
    "from peerselect import impartial\n",
    "from peerselect import profile_generator\n",
    "from peerselect.estimate_eps import estimate_eps\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('seaborn-whitegrid')\n",
    "\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'cm'\n",
    "matplotlib.rcParams['font.family'] = 'STIXGeneral'\n",
    "matplotlib.rcParams['font.size'] = 25\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Impartial:\n",
    "    VANILLA = \"Vanilla\"\n",
    "    EXACT = \"ExactDollarPartition\"\n",
    "    PARTITION = \"Partition\"\n",
    "    DPR = \"DollarPartitionRaffle\"\n",
    "    CREDIABLE = \"CredibleSubset\"\n",
    "    RAFFLE = \"DollarRaffle\"\n",
    "    NOMINATION = \"PeerNomination\"\n",
    "    ALL = (VANILLA, EXACT, PARTITION, RAFFLE, CREDIABLE, DPR, NOMINATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate an instance of the peer review  problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n = 120\n",
    "k = 20\n",
    "m = 7\n",
    "l = 4\n",
    "p = 0.8\n",
    "\n",
    "# The agents are represented by integers that conveniently represent the ground truth\n",
    "agents = np.arange(0, n)\n",
    "# Generate individual rankings of agents (the voting profile) using Mallows model. \n",
    "# It is possible to mix different ground truths and different dispersion parameters\n",
    "profile = profile_generator.generate_mallows_mixture_profile(agents, agents, [p, 1-p], [agents, agents], [0.5, 1.0])\n",
    "# Generate the clustering into l clusters for the EDP assignment.\n",
    "clustering = impartial.even_partition_order(sorted(agents, key=lambda j: random.random()), l)\n",
    "\n",
    "# We need some ways to convert rankings into scores for EDP:\n",
    "# - Borda scores -- need to start at 1 to distinguish from non-review in the score matrix\n",
    "scores = np.arange(m, 0, -1)\n",
    "# - Approval scores a-la PeerNomination\n",
    "nom_quota = (k/n)*m\n",
    "scores_pn = np.concatenate((np.ones(math.floor(nom_quota)),\n",
    "        [nom_quota-math.floor(nom_quota)],\n",
    "        np.zeros(m-math.ceil(nom_quota))))\n",
    "\n",
    "# Generate an m-regular assignment\n",
    "m_assignment = profile_generator.generate_approx_m_regular_assignment(agents, m, clustering, randomize=False)\n",
    "\n",
    "# Combine the assignment and profile into a score matrix, refer to code for format. \n",
    "score_matrix = profile_generator.strict_m_score_matrix(profile, m_assignment, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can run the algorithms\n",
    "The output of each algorithm is simply the list of accepted (winning) agents\n",
    "\n",
    "Some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDP\n",
    "impartial.exact_dollar_partition_explicit(score_matrix, k, clustering, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to use the slack parameter for PeerNomination. Use this function to get the best estimate.\n",
    "eps_estimate = estimate_eps(n, m, k)\n",
    "\n",
    "# Run PeerNomination without weights\n",
    "impartial.peer_nomination_lottery(score_matrix, k, eps_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PeerNomination with Distance. It usually returns too many agents so can set slack parameter to 0 as a heuristic. \n",
    "impartial.weighted_peer_nomination(score_matrix, k, impartial.dist_weights, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Testing the Algorithms\n",
    " Here's a simple way to run some experiments and collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_DEBUG = False\n",
    "\n",
    "#random.seed(15)\n",
    "\n",
    "# Warning! It's quite slow for a big number of parameters or iterations. \n",
    "# For the paralellised version, use scripts in the experiment folder.\n",
    "\n",
    "# Number of iterations for each combination of parameters\n",
    "s = 10\n",
    "# Range of parameters for testing\n",
    "test_n = [120]\n",
    "test_k = [20, 25, 30]\n",
    "test_m = [7, 8, 9]\n",
    "test_l = [4]\n",
    "test_p = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.9, 1]\n",
    "\n",
    "# Map for all Results.\n",
    "gt_results = {}\n",
    "pn_sizes = {}  #track output size of peer nomination\n",
    "for n,k,m,l,p in itertools.product(test_n, test_k, test_m, test_l, test_p):\n",
    "    agents = np.arange(0, n)\n",
    "    \n",
    "    eps_estimate = estimate_eps(n, m, k)\n",
    "    \n",
    "    for c_sample in range(s):\n",
    "        # Generate a profile and clustering\n",
    "        profile = profile_generator.generate_mallows_mixture_profile(agents, agents, [p, 1-p], [agents, agents], [0.5, 1])\n",
    "        clustering = impartial.even_partition_order(sorted(agents, key=lambda j: random.random()), l)\n",
    "        \n",
    "        # Borda -- need to start at 1 to distinguish from non-review in the score matrix\n",
    "        scores = np.arange(m, 0, -1)\n",
    "        \n",
    "        # Approval a-la PeerNomination\n",
    "        nom_quota = (k/n)*m\n",
    "        scores_pn = np.concatenate((np.ones(math.floor(nom_quota)),\n",
    "                [nom_quota-math.floor(nom_quota)],\n",
    "                np.zeros(m-math.ceil(nom_quota))))\n",
    "        \n",
    "        # Generate an m-regular assignment\n",
    "        m_assignment = profile_generator.generate_approx_m_regular_assignment(agents, m, clustering, randomize=False)\n",
    "        \n",
    "        score_matrix = profile_generator.strict_m_score_matrix(profile, m_assignment, scores)\n",
    "        score_matrix_pn = profile_generator.strict_m_score_matrix(profile, m_assignment, scores_pn)\n",
    "        \n",
    "        # Capture the winning sets\n",
    "        ws = {}\n",
    "\n",
    "        # Run peer nomination using the estimated epsilon\n",
    "        ws[Impartial.NOMINATION] = impartial.peer_nomination_lottery(score_matrix, k, eps_estimate)\n",
    "        ws[Impartial.EXACT] = impartial.exact_dollar_partition_explicit(score_matrix, k, clustering, normalize=True)\n",
    "        ws[\"EDP_rank\"] = impartial.exact_dollar_partition_explicit(score_matrix_pn, k, clustering, normalize=True)\n",
    "        ws[\"PN_dist\"] = impartial.weighted_peer_nomination(score_matrix, k, impartial.dist_weights, eps_estimate)\n",
    "        ws[\"PN_maj\"] = impartial.weighted_peer_nomination(score_matrix, k, impartial.maj_weights, eps_estimate)\n",
    "        ws[\"PN_step\"] = impartial.weighted_peer_nomination(score_matrix, k, impartial.step_weights, eps_estimate)\n",
    "\n",
    "        for x in [Impartial.NOMINATION, Impartial.EXACT, \"EDP_rank\", \"PN_dist\", \"PN_maj\", \"PN_step\"]:\n",
    "            key = (n, k, m, l, p, s, x)\n",
    "            gt_results[key] = gt_results.get(key, []) + [len(set(np.arange(0, k)) & set(ws[x]))]\n",
    "            pn_sizes[key] = pn_sizes.get(key, []) + [len(set(ws[x]))]\n",
    "\n",
    "        key = (n, k, m, l, p)\n",
    "        \n",
    "    print(\"Finished: \" + \",\".join([str(x) for x in [n, k, m, l, p, s]]))\n",
    "    \n",
    "print(\"All done!\")\n",
    "\n",
    "df = pd.DataFrame(gt_results)\n",
    "df.columns.names = ['n', 'k', 'm', 'l', 'p', 's', 'algo']\n",
    "\n",
    "df_sizes = pd.DataFrame(pn_sizes)\n",
    "df_sizes.columns.names = ['n', 'k', 'm', 'l', 'p', 's', 'algo']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and loading the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"peerselect/db/some_exp.pkl\") # saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"peerselect/db/some_exp.pkl\") # loading the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the data for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabels = [0.5, 1]\n",
    "\n",
    "df_filtered = df[\n",
    "    [x for x in df.columns if x[4] in xlabels]\n",
    "]\n",
    "df_sizes = df_sizes[\n",
    "    [x for x in df_sizes.columns if x[4] in xlabels]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Takes a slice of the array and generates the graphs etc.\n",
    "#\n",
    "def make_output(df, df_sizes, test, labels, save_path=None):\n",
    "    means = df.loc[:, test].mean().unstack()\n",
    "    errors = df.loc[:, test].std().unstack()\n",
    "    mins = df.loc[:, test].min().unstack()\n",
    "\n",
    "#     algo_list = [Impartial.NOMINATION, \"PN_dist\", \"PN_step\", \"PN_maj\", Impartial.EXACT]\n",
    "    algo_list = np.unique([x[-1] for x in df.columns])\n",
    "\n",
    "    means = means[algo_list]\n",
    "    errors = errors[algo_list]\n",
    "    \n",
    "    for index, row in means.iterrows():\n",
    "        means.loc[index] = row / float(index[1])\n",
    "    for index, row in errors.iterrows():\n",
    "        errors.loc[index] = row / float(index[1])\n",
    "    \n",
    "    #Set colors..\n",
    "#     color_list = plt.cm.Paired(np.linspace(0, 1, 10))\n",
    "#     color_list = color_list[:7]\n",
    "    color_list = [\"#7fc97f\",\"#beaed4\",\"#fdc086\",\"#ffff99\",\"#386cb0\"]\n",
    "    color_list = color_list[1:5]\n",
    "    #color_list = sns.color_palette(\"pastel\", 6)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "    \n",
    "    \n",
    "    means.plot(kind='bar', ax=ax1, legend=False, yerr=errors.values.T, figsize=(20, 10),\n",
    "            color=color_list, error_kw={'ecolor':'Black', 'linewidth':2, 'capsize':4}, width=0.85, edgecolor=\"k\")\n",
    "    ax1.set_ylabel(\"Recall\")    \n",
    "    ax1.set_ylim([0, 1])\n",
    "    ax1.set_yticks(np.linspace(0, 1, 5))\n",
    "    \n",
    "#     print(means)\n",
    "    \n",
    "#     if df_sizes != None:\n",
    "    means = df_sizes.loc[:, test].mean().unstack()\n",
    "    errors = df_sizes.loc[:, test].std().unstack()\n",
    "    mins = df_sizes.loc[:, test].min().unstack()\n",
    "\n",
    "#     algo_list = [Impartial.NOMINATION, \"PN_maj\",\"PN_dist\",  \"PN_step\", Impartial.EXACT]\n",
    "\n",
    "    means = means[algo_list]\n",
    "    errors = errors[algo_list]\n",
    "\n",
    "    for index, row in means.iterrows():\n",
    "        means.loc[index] = row / float(index[1])\n",
    "    for index, row in errors.iterrows():\n",
    "        errors.loc[index] = row / float(index[1])\n",
    "\n",
    "    means.plot(kind='bar', ax=ax2, legend=False, yerr=errors.values.T, figsize=(20, 10),\n",
    "            color=color_list, error_kw={'ecolor':'Black', 'linewidth':2, 'capsize':4}, width=0.85, edgecolor=\"k\")\n",
    "#     plt.title(f\"Adversarial Fair Test, n={test[0]}, k={test[1]}, m={test[2]}, l={test[3]}\")\n",
    "#     plt.legend([\"PN+Unit\", \"PN+Dist\", \"PN+Step\", \"PN+Maj\", \"EDP\"], bbox_to_anchor = (0,-0.05,1,1), bbox_transform=plt.gcf().transFigure, loc='lower center', ncol=7, borderaxespad=0.)\n",
    "\n",
    "    ax2.set_ylabel(\"Relative Size\")\n",
    "    ax2.set_xlabel(\"Proportion of accurate reviewers\")\n",
    "#     ax2.axhline(1, c=\"0.5\", ls=\"--\")\n",
    "    \n",
    "    print()\n",
    "    plt.legend(algo_list, bbox_to_anchor = (0,-0.05,1,1), bbox_transform=plt.gcf().transFigure, loc='lower center', ncol=7, borderaxespad=0.)\n",
    "    plt.gca().set_xticklabels(labels)\n",
    "    plt.xticks(rotation = 0)\n",
    "    plt.yticks(np.linspace(0, 1, 5))\n",
    "\n",
    "    if save_path != None:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\", dpi=400)    \n",
    "    else:\n",
    "        plt.show()\n",
    "#     print(means)\n",
    "    return means\n",
    "\n",
    "make_output(df, df_sizes, (120, 25, 8, 4, slice(None), 1), xlabels)\n",
    "# Tbh I don't really know how this works, I just modified it enough to show the plots I want."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
